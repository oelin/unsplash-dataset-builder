{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "29xOci7morT5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsplash Dataset Builder"
      ],
      "metadata": {
        "id": "jSNfGRzHon-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "29xOci7morT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "IP0_ddKholI-",
        "outputId": "b00661b1-810e-4e84-8f33-a87ee9989356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@markdown ğŸ“¦ install dependencies.\n",
        "\n",
        "!pip -q install datasets pillow requests tqdm huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ§© import dependencies.\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "from typing import Iterable\n",
        "from urllib.parse import quote\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from PIL import Image\n",
        "import requests\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iof87xyoqnuw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset"
      ],
      "metadata": {
        "id": "_h1OBZjWsFK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“š create the dataset.\n",
        "\n",
        "UNSPLASH_SEARCH_URL = 'https://unsplash.com/napi/search/photos?page={}&per_page=10&plus=none&query={}&xp=semantic-search%3Aexperiment'\n",
        "\n",
        "queries = \"portrait, flash photography, people\" # @param {type:\"string\"}\n",
        "queries = queries.split(',')\n",
        "queries = [query.strip() for query in queries]\n",
        "\n",
        "limits = \"10, 10, 10\" # @param {type:\"string\"}\n",
        "limits = limits.split(',')\n",
        "limits = [int(limit.strip()) for limit in limits]\n",
        "\n",
        "dataset_dictionary = {'url': [], 'text': []}\n",
        "result_urls = {}\n",
        "\n",
        "for i, (query, limit) in enumerate(zip(queries, limits)):\n",
        "    page = 0\n",
        "    result_count = 0\n",
        "    progress_bar = tqdm(total=limit)\n",
        "\n",
        "    print(f'ğŸ” scraping query {i:03d}/{len(queries):03d} ({query})...')\n",
        "\n",
        "    while result_count < limit:\n",
        "        response = requests.get(UNSPLASH_SEARCH_URL.format(page, quote(query)))\n",
        "        page += 1\n",
        "\n",
        "        try:\n",
        "            results = json.loads(response.text)['results']\n",
        "        except:\n",
        "            print(f'ğŸ™Š warning: page {page:04d} returned invalid JSON, skipping...')\n",
        "            continue\n",
        "\n",
        "        if not results:\n",
        "            break\n",
        "\n",
        "        for j, result in enumerate(results):\n",
        "            try:\n",
        "                result_url = result['urls']['regular']\n",
        "                result_text = result['alt_description'].strip()\n",
        "\n",
        "                assert result_url and result_text\n",
        "                assert result_url not in result_urls\n",
        "\n",
        "                dataset_dictionary['url'].append(result_url)\n",
        "                dataset_dictionary['text'].append(result_text)\n",
        "                result_urls[result_url] = True\n",
        "                result_count += 1\n",
        "                progress_bar.update(1)\n",
        "\n",
        "                if result_count >= limit:\n",
        "                    progress_bar.close()\n",
        "                    break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "print('ğŸ“š creating dataset...')\n",
        "\n",
        "dataset = Dataset.from_dict(dataset_dictionary, split='train')\n",
        "dataset = dataset.filter(lambda example: example)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "print('âœ¨ finished!')\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "PvPt92Adr0V8",
        "cellView": "form"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown â« upload the dataset to Hugging Face.\n",
        "\n",
        "dataset_path = 'username/repo'  #@param {type:\"string\"}\n",
        "\n",
        "dataset.push_to_hub(dataset_path, split='train')"
      ],
      "metadata": {
        "id": "J_cSNJ9pb40B",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}
